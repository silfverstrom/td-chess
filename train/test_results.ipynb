{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkchess.evaluators import get_training_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import chess\n",
    "import timeit\n",
    "from linkchess.evaluators import get_training_data, get_nnue_training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"/Users/silfverstrom/Workspace/link/projects/td-chess/output/nnue_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(board, model):\n",
    "    x1, x2, x3 = get_nnue_training_data(board)\n",
    "\n",
    "    x1 = np.array([x1])\n",
    "    x2 = np.array([x2])\n",
    "    x3 = np.array([x3])\n",
    "    out = model([x1, x2, x3])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def p(fen):\n",
    "    score =  predict(chess.Board(fen), model)\n",
    "    print(score, logit(score))\n",
    "def logit(x):\n",
    "    return - tf.math.log(1. / x - 1.) * 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.12412447]], shape=(1, 1), dtype=float32) tf.Tensor([[-1172.3634]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p('rnbqk1nr/p1pp1ppp/8/2b1p3/2p1P3/5N2/PPPP1PPP/RNBQ1RK1 w kq - 0 5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.16469827]], shape=(1, 1), dtype=float32) tf.Tensor([[-974.2067]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p('rnbqk1nr/pppp1ppp/8/2b1p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.17862776]], shape=(1, 1), dtype=float32) tf.Tensor([[-915.4034]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p('rnbqk1nr/pppp1ppp/8/2b1p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.13238774]], shape=(1, 1), dtype=float32) tf.Tensor([[-1128.006]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p('rnbqk1nr/pppp1ppp/8/2b1N3/4P3/8/PPPP1PPP/RNBQKB1R b KQkq - 0 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.09885545]], shape=(1, 1), dtype=float32) tf.Tensor([[-1326.0042]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p('rnb1k1nr/pppp1ppp/5q2/2b1N3/4P3/2N5/PPPP1PPP/R1BQKB1R b KQkq - 2 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5.6922436e-06]], shape=(1, 1), dtype=float32) tf.Tensor([[-7245.8403]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p('rnb1k1nr/pppp1ppp/2N5/2b5/4P3/2N5/PPPP1PPP/R1BQKB1R b KQkq - 0 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.8917179]], shape=(1, 1), dtype=float32) tf.Tensor([[1265.046]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p('rnb1k1nr/p1pp1ppp/1pN5/2b5/4P3/2N5/PPPP1PPP/R1BQKB1R w KQkq - 0 6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.00329533]], shape=(1, 1), dtype=float32) tf.Tensor([[nan]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p('rnb1k1nr/p1pp1ppp/1p6/2b1N3/4P3/2N5/PPPP1PPP/R1BQKB1R b KQkq - 1 6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.01177791]], shape=(1, 1), dtype=float32) tf.Tensor([[-2657.809]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.7899803]], shape=(1, 1), dtype=float32) tf.Tensor([[794.88403]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.01374525]], shape=(1, 1), dtype=float32) tf.Tensor([[-2563.9329]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.78937066]], shape=(1, 1), dtype=float32) tf.Tensor([[792.6816]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# black better\n",
    "p('rnbqkb1r/pppp1ppp/8/4p2n/4P3/2N5/PPPP1PPP/R1B1KBNR w KQkq - 0 4') # white turn\n",
    "p('rnbqkb1r/pppp1ppp/8/4p2n/4P3/2NP4/PPP2PPP/R1B1KBNR b KQkq - 0 4') # black turn\n",
    "p('rnbqk2r/pppp1ppp/8/2b1p2n/4P3/2NP4/PPP2PPP/R1B1KBNR w KQkq - 1 5') # white turn\n",
    "p('rnbqk2r/pppp1ppp/8/2b1p2n/4P3/2NP4/PPPB1PPP/R3KBNR b KQkq - 2 5') # black turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.16640256]], shape=(1, 1), dtype=float32) tf.Tensor([[-966.8044]], shape=(1, 1), dtype=float32)\n",
      "g1h3\n",
      "tf.Tensor([[0.20781656]], shape=(1, 1), dtype=float32) tf.Tensor([[-802.8824]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.17184642]], shape=(1, 1), dtype=float32) tf.Tensor([[-943.5585]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "board = chess.Board()\n",
    "p(board.fen())\n",
    "move = list(board.legal_moves)[0]\n",
    "print(move)\n",
    "board.push(move)\n",
    "p(board.fen())\n",
    "board.turn = True\n",
    "p(board.fen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-eb42ca6e4af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"/tmp/tdchess_checkpoint/model.01-0.30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f73fa210e6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'weights'"
     ]
    }
   ],
   "source": [
    "model.layers.weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import model as M\n",
    "import numpy\n",
    "import struct\n",
    "\n",
    "def coalesce_weights(weights):\n",
    "  # incoming weights are in [256][INPUTS]\n",
    "  print('factorized shape:', weights.shape)\n",
    "  k_base = 41024\n",
    "  p_base = k_base + 64\n",
    "  result = []\n",
    "  # Goal here is to add together all the weights that would be active for the\n",
    "  # given piece and king position in the halfkp inputs.\n",
    "  for i in range(k_base):\n",
    "    k_idx = i // 256\n",
    "    p_idx = i % 256\n",
    "    w = weights.narrow(1, i, 1).clone()\n",
    "    # TODO - divide by 20 to approximate # of pieces on the board, but this is\n",
    "    # a huge hack.  Issue is there is only one king position set in the factored\n",
    "    # positions, but we add it's weights to the # of pieces on the board.  This\n",
    "    # vastly overweights the king value.\n",
    "    w = w + weights.narrow(1, k_base + k_idx, 1) / 20\n",
    "    if p_idx > 0:\n",
    "      w = w + weights.narrow(1, p_base + p_idx - 1, 1)\n",
    "    result.append(w)\n",
    "  return torch.cat(result, dim=1)\n",
    "\n",
    "class NNUEWriter():\n",
    "  \"\"\"\n",
    "  All values are stored in little endian.\n",
    "  \"\"\"\n",
    "  def __init__(self, model):\n",
    "    self.buf = bytearray()\n",
    "\n",
    "    self.write_header()\n",
    "    self.int32(0x5d69d7b8) # Feature transformer hash\n",
    "    selftmma.write_feature_transformer(model.input)\n",
    "    self.int32(0x63337156) # FC layers hash\n",
    "    self.write_fc_layer(model.l1)\n",
    "    self.write_fc_layer(model.l2)\n",
    "    self.write_fc_layer(model.output, is_output=True)\n",
    "\n",
    "  def write_header(self):\n",
    "    self.int32(0x7AF32F16) # version\n",
    "    self.int32(0x3e5aa6ee) # halfkp network hash\n",
    "    description = b\"Features=HalfKP(Friend)[41024->256x2],\"\n",
    "    description += b\"Network=AffineTransform[1<-32](ClippedReLU[32](AffineTransform[32<-32]\"\n",
    "    description += b\"(ClippedReLU[32](AffineTransform[32<-512](InputSlice[512(0:512)])))))\"\n",
    "    self.int32(len(description)) # Network definition\n",
    "    self.buf.extend(description)\n",
    "\n",
    "  def write_feature_transformer(self, layer):\n",
    "    # int16 bias = round(x * 127)\n",
    "    # int16 weight = round(x * 127)\n",
    "    bias = layer.bias.data\n",
    "    bias = bias.mul(127).round().to(torch.int16)\n",
    "    print('ft bias:', numpy.histogram(bias.numpy()))\n",
    "    self.buf.extend(bias.flatten().numpy().tobytes())\n",
    "\n",
    "    weight = layer.weight.data\n",
    "    weight = coalesce_weights(weight)\n",
    "    weight = weight.mul(127).round().to(torch.int16)\n",
    "    print('ft weight:', numpy.histogram(weight.numpy()))\n",
    "    # weights stored as [41024][256], so we need to transpose the pytorch [256][41024]\n",
    "    self.buf.extend(weight.transpose(0, 1).flatten().numpy().tobytes())\n",
    "\n",
    "  def write_fc_layer(self, layer, is_output=False):\n",
    "    # FC layers are stored as int8 weights, and int32 biases\n",
    "    kWeightScaleBits = 6\n",
    "    kActivationScale = 127.0\n",
    "    if not is_output:\n",
    "      kBiasScale = (1 << kWeightScaleBits) * kActivationScale # = 8128\n",
    "    else:\n",
    "      kBiasScale = 9600.0 # kPonanzaConstant * FV_SCALE = 600 * 16 = 9600\n",
    "    kWeightScale = kBiasScale / kActivationScale # = 64.0 for normal layers\n",
    "    kMaxWeight = 127.0 / kWeightScale # roughly 2.0\n",
    "\n",
    "    # int32 bias = round(x * kBiasScale)\n",
    "    # int8 weight = round(x * kWeightScale)\n",
    "    bias = layer.bias.data\n",
    "    bias = bias.mul(kBiasScale).round().to(torch.int32)\n",
    "    print('fc bias:', numpy.histogram(bias.numpy()))\n",
    "    self.buf.extend(bias.flatten().numpy().tobytes())\n",
    "    weight = layer.weight.data\n",
    "    weight = weight.clamp(-kMaxWeight, kMaxWeight).mul(kWeightScale).round().to(torch.int8)\n",
    "    print('fc weight:', numpy.histogram(weight.numpy()))\n",
    "    # Stored as [outputs][inputs], so we can flatten\n",
    "    self.buf.extend(weight.flatten().numpy().tobytes())\n",
    "\n",
    "  def int32(self, v):\n",
    "    self.buf.extend(struct.pack(\"<i\", v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(40960, 256) dtype=float32, numpy=\n",
      "array([[-0.00490437,  0.00235895, -0.009206  , ...,  0.00328556,\n",
      "         0.01067283,  0.00791233],\n",
      "       [ 0.00919103, -0.00273866, -0.00635432, ..., -0.01160318,\n",
      "         0.00649968,  0.00671539],\n",
      "       [ 0.00834631,  0.00395545,  0.00502597, ...,  0.00758409,\n",
      "        -0.00382586,  0.00545456],\n",
      "       ...,\n",
      "       [ 0.00566082, -0.01132983, -0.00727855, ...,  0.01054286,\n",
      "         0.00739815, -0.00231866],\n",
      "       [-0.00767518, -0.01052778,  0.00989245, ...,  0.00326584,\n",
      "         0.00284068, -0.00750623],\n",
      "       [ 0.0089065 ,  0.00151161, -0.00636951, ..., -0.00529038,\n",
      "        -0.00919625, -0.00720907]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([ 5.61658177e-04,  1.36853410e-02,  1.90509558e-02,  2.85375062e-02,\n",
      "       -2.30192039e-02,  5.12517337e-03, -3.13058533e-02, -9.24749672e-03,\n",
      "       -2.01973282e-02, -2.02356372e-02, -1.66795235e-02, -3.44015919e-02,\n",
      "       -1.42752519e-02,  3.08358087e-03,  1.01964725e-02, -3.18357581e-03,\n",
      "        2.00372487e-02,  5.71024325e-03,  1.24059413e-02, -2.56957505e-02,\n",
      "        1.24954628e-02, -1.60678430e-03,  3.24983150e-03,  1.24246711e-02,\n",
      "        1.62581187e-02, -7.45462673e-03,  3.08194384e-02, -4.66822367e-03,\n",
      "        1.53220203e-02,  2.71842852e-02,  1.33569888e-03,  1.68669643e-03,\n",
      "       -1.24725662e-02,  5.35454415e-03, -1.12741033e-03,  2.77549829e-02,\n",
      "        2.50419527e-02, -1.44248514e-03,  2.33419407e-02, -1.60106085e-03,\n",
      "        1.86832268e-02, -2.46634390e-02,  1.92705728e-02,  1.78133249e-02,\n",
      "       -4.98152291e-03,  4.70021879e-03,  3.07340082e-03,  7.96126202e-03,\n",
      "       -1.96964927e-02,  1.29994275e-02, -2.46599931e-02,  2.58545931e-02,\n",
      "        5.29210176e-03,  2.20821686e-02, -4.27958323e-03,  2.18335409e-02,\n",
      "        4.06465027e-03,  3.27325470e-05, -1.89069118e-02,  1.48673756e-02,\n",
      "       -1.22804865e-02,  1.86911598e-02, -4.56307363e-03,  8.87003168e-03,\n",
      "       -1.73860136e-02, -5.56858210e-03,  2.11185999e-02,  1.43524054e-02,\n",
      "        2.69081984e-02,  1.77724250e-02, -2.48773210e-02, -1.63431372e-02,\n",
      "        2.28986274e-02,  2.67841909e-02, -4.50394209e-03,  1.26839373e-02,\n",
      "        2.32447293e-02,  6.13070419e-03, -2.53832061e-02, -2.80809756e-02,\n",
      "       -6.96080644e-03,  2.29560840e-03,  2.35480592e-02, -2.22130753e-02,\n",
      "        9.00279917e-03, -3.56135494e-03, -2.66859848e-02,  1.88591983e-02,\n",
      "       -1.48527790e-02, -2.11243401e-03, -1.79895367e-02,  2.84494981e-02,\n",
      "       -9.77870659e-04, -2.09189337e-02,  2.08140183e-02, -5.93526661e-03,\n",
      "        1.14939176e-02, -8.79496336e-03, -5.02243638e-04, -2.14681122e-02,\n",
      "       -1.11762958e-03, -3.52654699e-03,  2.12241095e-02,  2.44595353e-02,\n",
      "       -4.66935942e-03,  3.06045404e-03,  2.12959312e-02, -1.01473113e-03,\n",
      "        1.55784665e-02,  2.36872453e-02, -1.59246894e-03, -2.69364170e-03,\n",
      "       -2.41436511e-02,  2.67703198e-02,  4.49325598e-05, -1.05731245e-02,\n",
      "       -2.72700004e-02, -1.69519950e-02,  1.10401977e-02,  1.40569499e-02,\n",
      "        3.50390887e-03,  2.57643405e-04, -1.61026176e-02, -1.37504982e-03,\n",
      "        2.67677358e-04,  3.94356390e-03,  6.23750908e-04, -1.21534122e-02,\n",
      "        1.20067699e-02, -2.82018504e-04, -2.61947867e-02,  8.78046546e-03,\n",
      "        2.32156962e-02, -1.80666111e-02,  4.79280483e-04,  1.85747258e-02,\n",
      "        1.01469848e-02,  1.02030197e-02, -2.78093554e-02,  1.51158236e-02,\n",
      "       -1.97148789e-02, -2.10630707e-02,  2.81673614e-02,  8.85611400e-03,\n",
      "       -1.97940525e-02,  2.36488804e-02, -2.21265107e-02, -2.30356604e-02,\n",
      "       -1.86288860e-04, -1.77088741e-03, -2.31905747e-02, -3.38696241e-02,\n",
      "       -9.51476302e-03, -2.75806175e-03, -5.24899689e-03, -2.17702743e-02,\n",
      "        5.52802067e-03, -3.74176330e-03, -6.91093085e-03,  4.70488565e-03,\n",
      "        2.46935058e-03, -1.92414597e-02, -2.56755371e-02,  1.78071838e-02,\n",
      "       -2.28957646e-02, -1.25826169e-02,  6.52328180e-03, -2.13797148e-02,\n",
      "        1.66439302e-02, -1.32908691e-02, -1.56883933e-02,  2.55939038e-03,\n",
      "        1.46281614e-03, -2.21411772e-02,  8.77232384e-03,  2.66535953e-02,\n",
      "       -4.19845193e-04, -1.79546773e-02, -3.42343515e-03, -2.04946548e-02,\n",
      "       -7.93260988e-03,  2.02842075e-02,  1.56500600e-02, -5.90399792e-03,\n",
      "        2.35328972e-02, -1.38473809e-02, -7.47242151e-03, -1.15822442e-03,\n",
      "        7.63142668e-03,  5.26637631e-03, -8.18466861e-03,  5.54771256e-03,\n",
      "        3.22139822e-02, -2.89063808e-03, -2.87632160e-02, -1.49207772e-03,\n",
      "        2.36742701e-02, -5.78597980e-03, -2.12638336e-03, -2.43116096e-02,\n",
      "        2.98088067e-03, -1.55516416e-02,  2.26218975e-03,  2.73595210e-02,\n",
      "        1.71640962e-02, -2.05395017e-02, -2.31165234e-02, -9.76625271e-03,\n",
      "       -6.50406437e-05,  3.12778205e-02, -1.48927700e-02,  1.01885917e-02,\n",
      "       -3.40008549e-03,  5.32374345e-03,  1.17542138e-02, -5.32093830e-03,\n",
      "        5.06608048e-03, -2.77281366e-03,  3.21757840e-03, -3.04307044e-03,\n",
      "       -2.53164396e-03,  2.07821652e-02,  1.13380793e-02,  2.03356948e-02,\n",
      "       -9.09766555e-03, -2.41834391e-03, -5.13553200e-03, -1.13648837e-02,\n",
      "       -1.84721909e-02, -1.37155624e-02,  3.64766479e-03,  2.10874081e-02,\n",
      "        1.33710876e-02, -6.55960466e-04,  9.99833690e-04,  1.68679599e-02,\n",
      "        2.42803097e-02, -2.29029432e-02, -7.44068902e-03, -5.61855501e-03,\n",
      "        8.82421434e-03,  6.39866805e-03,  1.31097492e-02, -5.68218483e-03,\n",
      "        2.79380418e-02,  1.02181695e-02,  2.81943697e-02, -2.26282445e-03,\n",
      "       -1.06597319e-02, -2.02893466e-02, -3.07068974e-03,  2.81719421e-03,\n",
      "        2.14869007e-02,  1.93526652e-02, -2.86287889e-02,  2.12359671e-02],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class NNUEWriter():\n",
    "  \"\"\"\n",
    "  All values are stored in little endian.\n",
    "  \"\"\"\n",
    "  def __init__(self, model):\n",
    "    self.buf = bytearray()\n",
    "\n",
    "    self.write_header()\n",
    "    self.int32(0x5d69d7b8) # Feature transformer hash\n",
    "    self.write_feature_transformer(model.layers[2])\n",
    "\n",
    "  def write_header(self):\n",
    "    self.int32(0x7AF32F16) # version\n",
    "    self.int32(0x3e5aa6ee) # halfkp network hash\n",
    "    description = b\"Features=HalfKP(Friend)[41024->256x2],\"\n",
    "    description += b\"Network=AffineTransform[1<-32](ClippedReLU[32](AffineTransform[32<-32]\"\n",
    "    description += b\"(ClippedReLU[32](AffineTransform[32<-512](InputSlice[512(0:512)])))))\"\n",
    "    self.int32(len(description)) # Network definition\n",
    "    self.buf.extend(description)\n",
    "  def write_feature_transformer(self, layer):\n",
    "    print(layer.weights)\n",
    "\n",
    "  def int32(self, v):\n",
    "    self.buf.extend(struct.pack(\"<i\", v))\n",
    "    \n",
    "w = NNUEWriter(model) # traner + 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
